library(maptools)
gpclibPermit()
install.packages("maptools")
library(maptools)
gpclibPermit()
nz <- readShapeSpatial("NZL_adm0")
plot(nz)
nz <- readShapeSpatial("NZL_adm1")
plot(nz)
install.packages("leaflet")
m <- leaflet() %>%
setView(lng = 145.0431, lat = -37.8773, zoom = 15) %>%
addTiles()
m
install.packages("leaflet")
library(leaflet)
m <- leaflet() %>%
setView(lng = 145.0431, lat = -37.8773, zoom = 15) %>%
addTiles()
m
m <- leaflet()
m <- setView(m, lng = 145.0431, lat = -37.8773, zoom = 15)
m <- addTiles(m)
m
m %>% addProviderTiles("Stamen.Toner")
g <- graph.formula
library(igraph)
install.packages("igraph")
library(igraph)
g <- graph.formula(1-2,1-3,2-3,2-4)
plot(g)
diameter(g)
get.adjacency(g)
V(g)$name <- c("Bruce","Clark","Diana","Peter")
plot(g, vertex.label = V(g)$name)
get.adjacency(g)
dg <- graph.formula(1-+2, 1-+3,2-+3.2-+4,3-+2)
plot(dg)
dg <- graph.formula(1-+2, 1-+3,2-+3.2-+4)
plot(dg)
dg <- graph.formula(1-+2, 1-+3,2-+3,2-+4)
plot(dg)
dg <- graph.formula(1-+2, 1-+3,2-+3,2-+4,3-+2)
plot(dg)
dg <- graph.formula(1-+2, 1-+3,2-+3,2-+4)
plot(dg)
V(dg)$name <- c("Bruce","Clark","Diana","Peter")
plot(dg, vertex.label = V(g)$name)
is.weighted(dg)
wdg<- dg
E(wdg)$weights <- runif(ecount(wdg)) *1000
plot(wdg, vertex.label = V(wdgg)$name, edge.width = E(wdg)$weights)
plot(wdg, vertex.label = V(wdg)$name, edge.width=E(wdg)$weights / 100)
plot(wdg, vertex.label = V(wdg)$name, edge.width=E(wdg)$weights / 100)
G <- graph( c(1,2,1,3,1,4,3,4,3,5,5,6,6,7,7,8,8,9,3,8,5,8), directed = F )
G$name <- "Change my layout, I dare you"
# Assign attributes to the graph's vertices
V(G)$name  <- toupper(letters[1:9])
V(G)$color <- sample(rainbow(9),9,replace=FALSE)
# Assign attributes to the edges
E(G)$weight <- runif(length(E(G)),.5,4)
# Plot the graph
plot(G, layout = layout.auto,
main = G$name,
vertex.label = V(G)$name,
vertex.size = 25,
vertex.color= V(G)$color,
vertex.frame.color= "white",
vertex.label.color = "white",
vertex.label.family = "sans",
edge.width=E(G)$weight,
edge.color="black")
par(mfrow=c(2,2)) # 2 x 2 display
g <- graph.full(n=5, directed = FALSE, loops = FALSE)
plot(g)
g <- graph.star(n=5, mode="out")
plot(g)
g <- graph.star(n=5, mode="in")
plot(g)
g = graph.ring(n=5)
plot(g)
par(mfrow=c(1,1))
g <- graph.full(5)
E(g)$weight <- runif(ecount(g)) # random weights, run again for different result
E(g)$width <- 1
E(g)$color <- "red"
E(g)[ weight < 0.5 ]$width <- 2
E(g)[ weight < 0.5 ]$color <- "blue"
plot(g, layout=layout.circle, edge.width=E(g)$width, edge.color= E(g)$color)
E(g)$weight
g1 <- make_star(5)
g2 <- induced_subgraph(g1, 1:2) # select vertices
g3 <- subgraph.edges(g1, 1:3, TRUE) # select edges
par(mfrow=c(1,3))
plot(g1)
plot(g2)
plot(g3)
library(igraphdata)
data(karate) # load the built-in graph data
?karate
V(karate)
E(karate)
get.adjacency(karate)
install.packages("igraphdata")
library(igraphdata)
data(karate) # load the built-in graph data
?karate
V(karate)
E(karate)
get.adjacency(karate)
# Reproducible layout
set.seed(42)
# Now decorate, starting with labels and shapes
V(karate)$label <- sub("Actor ", "", V(karate)$name)
V(karate)$shape <- "circle"
# Differentiate two factions by color.
V(karate)[Faction == 1]$color <- "red"
V(karate)[Faction == 2]$color <- "dodgerblue"
V(karate)$size <- 4*sqrt(graph.strength(karate))
V(karate)$size2 <- V(karate)$size * .5
# Weight edges by number of common activities
E(karate)$width <- E(karate)$weight
F1 <- V(karate)[Faction==1]
F2 <- V(karate)[Faction==2]
E(karate)[ F1 %--% F1 ]$color <- "pink" # F1 to F1 i.e. internal
E(karate)[ F2 %--% F2 ]$color <- "lightblue"
E(karate)[ F1 %--% F2 ]$color <- "yellow"
V(karate)$label.dist <- ifelse(V(karate)$size >= 10, 0, 0.75)
l <- layout.kamada.kawai(karate)
plot(karate, layout=l)
install.packages("sand")
install.packages("sna")
install.packages("network")
library(sand) # Statistical Analysis of Network Data with R
library(sna) # Tools for Social Network Analysis
library(network)
A = get.adjacency(karate, sparse=FALSE)
g = network::as.network.matrix(A) # make a matrix
par(mfrow=c(1,1))
sna::gplot.target(
g, degree(g), main="Degree",
circ.lab = FALSE, # change to TRUE to see legend on concentric blue circles
circ.col="skyblue", usearrows = FALSE,
vertex.col=c("blue", rep("red", 32), "yellow"),
edge.col
A = get.adjacency(karate, sparse=FALSE)
g = network::as.network.matrix(A) # make a matrix
par(mfrow=c(1,1))
sna::gplot.target(
g, degree(g), main="Degree",
circ.lab = FALSE, # change to TRUE to see legend on concentric blue circles
circ.col="skyblue", usearrows = FALSE,
vertex.col=c("blue", rep("red", 32), "yellow"),
edge.col="darkgray"
)
par(mfrow=c(2,2))
par(mar=c(1.5, 1, 1, 0.5))
sna::gplot.target(
g, degree(g), main="Degree",
circ.lab = FALSE, # change to TRUE to see legend on concentric blue circles
circ.col="skyblue", usearrows = FALSE,
vertex.col=c("blue", rep("red", 32), "yellow"),
edge.col="darkgray"
)
sna::gplot.target(g, closeness(g), main="Closeness",
circ.lab = FALSE, circ.col="skyblue",
usearrows = FALSE,
vertex.col=c("blue", rep("red", 32), "yellow"),
edge.col="darkgray")
sna::gplot.target(g, betweenness(g), main="Betweenness",
circ.lab = FALSE, circ.col="skyblue",
usearrows = FALSE,
vertex.col=c("blue", rep("red", 32), "yellow"),
edge.col="darkgray")
sna::gplot.target(g, evcent(g), main="Eigenvalue",
circ.lab = FALSE, circ.col="skyblue",
usearrows = FALSE,
vertex.col=c("blue", rep("red", 32), "yellow"),
edge.col="darkgray")
# Importing libraries
library(ISLR)
library(psych)
library(ggplot2)
library(reshape2)
library(GGally)
library(lattice)
library(gridExtra)
library(leaps)
# we need the following libraries
library("tm") # for text mining
library("SnowballC") # for text stemming
library("wordcloud")
library("RColorBrewer")
# Read the text file
filePath <- "WordCloud.txt" # or get your own this is one of subject chapters
text <-readLines(filePath) # Load the data as a corpus and save a copy as 'input'
input <- Corpus(VectorSource(text))
docs <- input
# we need the following libraries
library("tm") # for text mining
library("SnowballC") # for text stemming
library("wordcloud")
library("RColorBrewer")
# Read the text file
filePath <- "WordCloud.txt" # or get your own this is one of subject chapters
text <-readLines(filePath) # Load the data as a corpus and save a copy as 'input'
input <- Corpus(VectorSource(text))
docs <- input
library("tm") # for text mining
library("SnowballC") # for text stemming
library("wordcloud")
library("RColorBrewer")
install.packages("wordcloud")
install.packages("tm")
install.packages("SnowballC")
library("tm") # for text mining
library("SnowballC") # for text stemming
library("wordcloud")
library("RColorBrewer")
# Read the text file
filePath <- "WordCloud.txt" # or get your own this is one of subject chapters
text <-readLines(filePath) # Load the data as a corpus and save a copy as 'input'
input <- Corpus(VectorSource(text))
docs <- input
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(
rowSums(m),
decreasing = TRUE
)
d <- data.frame(
words = names(v),
freq = v
)
nrow(d) # how many 'words'
head(d,10) # look at top 10
wordcloud(
words = d$words,
freq = d$freq,
min.freq = 1,
max.words = 200,
random.order = FALSE,
rot.per = 0.35,
colors = brewer.pal(8,"Dark2")
)
tail(d ,10)
# or look at all the words with
# d$word
# generate the raw word cloud, OK here because it's 'reasonably' tidy text
# up to you if you want to try this with e.g. tweetsset.seed(1234)
# so that the cloud is reproducible, optional
wordcloud(
words = d$words,
freq = d$freq,
min.freq = 1,
max.words = 200,
random.order = FALSE,
rot.per = 0.35,
colors = brewer.pal(8,"Dark2")
)
stopwords("english")
stopwords("french") # c'est la vie...
stopwords("romanian") # why not...
stopwords("swahili") # no... boo
docs <- tm_map(docs, tolower) # make all letters to lowercase
docs <- tm_map(docs, removeWords, stopwords("english"))
# this is destructive, you may need to start again or use the copy 'input'
# and process again
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(words = names(v), freq=v)
nrow(d)
head(d, 10)
# and display again, 'the' should be gone
set.seed(1234)
wordcloud(
words = d$words, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2")
)
stemCompletion(c("visualis","techniqu","identifi"), d$words)
stemCompletion(c("visualis","techniqu","identifi"), d$words, type = "prevalent")
stemCompletion(c("visualis","techniqu","identifi"), d$words, type = "random")
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(words = names(v),freq=v)
nrow(d)
head(d, 10)
set.seed(1234)
wordcloud(
words = d$words, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2")
)
# stemming first, input is original
docs <- tm_map(input, stemDocument, language = "english")
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(words = names(v),freq=v)
nrow(d)
head(d, 10)
set.seed(1234)
wordcloud(words = d$words, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# Remove common English stopwords (e.g. 'the')
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(words = names(v),freq=v)
nrow(d)
head(d, 10)
set.seed(1234)
wordcloud(words = d$words, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
findAssocs(dtm, terms = "kimbal", corlimit = 1) # ha (and note lowercase, must be automated)
findAssocs(dtm, terms = "text", corlimit = 0.5) #
require(httr) # may have to install these two
require(XML)
lemmatize <- function(wordlist) {
get.lemma <- function(word, url) {
response <- GET(url,query=list(spelling=word,standardize="",
wordClass="",wordClass2="",
corpusConfig="ncf",    # Nineteenth Century Fiction
media="xml"))
content <- httr::content(response, type="text/plain")
xml     <- xmlInternalTreeParse(content)
return(xmlValue(xml["//lemma"][[1]]))
}
# here's the code
url <- "http://classify.at.northwestern.edu/maserver/lemmatizer"
return(sapply(wordlist, get.lemma, url=url))
}
words <- c("is","am","was","are")
lemmatize(words)
# is am  was  are
# "be" "be" "be" "be"
# 'to be'
words <- c(
"walk","walking","walked","walk",
"Walker", "walks", "Walker Texas Ranger",
"sidewalk"
)
lemmatize(words)
words <- c("Walker Texas Ranger","range","ranger","ranged","rangy","Ranger")
# so ranger as in rang a bell!? <sigh> English </sigh>
lemmatize(words)
# load termDocMatrix
load("termDocMatrix.rdata")
# inspect part of the matrix
termDocMatrix[5:10,1:20]
# Transform Data into an Adjacency Matrix
# change it to a Boolean matrix
termDocMatrix[termDocMatrix>=1] <- 1
# transform into a term-term adjacency matrix
termMatrix <- termDocMatrix %*% t(termDocMatrix)
# inspect terms numbered 5 to 10
termMatrix[5:10,5:10]
library(igraph)
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels of vertices
V(g)$label <- V(g)$name
# set seed to make the layout reproducible
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g)
plot(g, layout=layout1)
# plot(g, layout=layout.kamada.kawai)
shiny::runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
runApp('proj1/dash')
setwd("~/proj1/dash")
data<- read.csv(champ
)
data<- read.csv(champ.txt
)
data11<- read.csv("champ.txt"
)
write.csv(data11, "champ.csv")
champ<- read.csv("champ.csv")
champ$Latitude<- as.numeric(champ$Latitude)
champ$Longitude<- as.numeric(champ$Longitude)
library(sp)
champ.sp<- SpatialPointsDataFrame(champ[,c(3,4)],champ[,c(3,4)])
champ$Latitude<- as.numeric(champ$Latitude)
champ$Longitude<- as.numeric(champ$Longitude)
View(champ)
View(champ)
champ.sp<- SpatialPointsDataFrame(champ[,c(4,5)],champ[,c(4,5)])
leaflet()%>%
addMarkers(data = champ.sp, lng= ~Longitude, lat = ~Latitude, popup = ~Team)%>%
addProviderTiles(providers$Stamen.Watercolor)%>%
setView(lng = -1.17, lat = 53.35, zoom = 7)
leaflet()%>%
addMarkers(data = champ.sp, lng= ~Longitude, lat = ~Latitude, popup = ~champ$Team)%>%
addProviderTiles(providers$Stamen.Watercolor)%>%
setView(lng = -1.17, lat = 53.35, zoom = 7)
leaflet()%>%
addMarkers(data = champ.sp, lng= ~Longitude, lat = ~Latitude, label = ~champ$Team)%>%
addProviderTiles(providers$Stamen.Watercolor)%>%
setView(lng = -1.17, lat = 53.35, zoom = 7)
runApp()
runApp()
View(champ)
View(champ)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
champ<- read.csv("champ.csv")
type(champ)
type(champ$Team)
typeof(champ)
typeof(champ$Team)
View(champ)
View(champ)
champ$Team
champ<-mutate(champ$Team = factor(champ$Team %in% c("Blackburn Rovers", "Arsenal", "Chelsea",
"Manchester United", "Manchester City",
"Leicester City", "Liverpool")))
champ<-mutate(champ$Team = factor(ifelse(champ$Team %in% c("Blackburn Rovers", "Arsenal", "Chelsea",
"Manchester United", "Manchester City",
"Leicester City"), champ$Team, "Liverpool")))
champ$Team["liverpool"]
champ$Team[liverpool]
champ$Team["liverpool"]
champ$Team[]
champ$Team[1]
champ<- read.csv("champ.csv")
champ$Latitude<- as.numeric(champ$Latitude)
champ$Longitude<- as.numeric(champ$Longitude)
champ<-mutate(champ$Team = factor(ifelse(champ$Team %in% c("Blackburn Rovers", "Arsenal", "Chelsea",
"Manchester United", "Manchester City",
"Leicester City"), champ$Team, "Liverpool")))
runApp()
runApp()
champ$Team
str(champ)
champ<- read.csv("champ.csv")
str(champ)
runApp()
runApp()
runApp()
runApp()
champ<- read.csv("champ.csv")
champ$Latitude<- as.numeric(champ$Latitude)
runApp('~/proj1')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
